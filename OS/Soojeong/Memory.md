### 운영체제 메모리 파트 정리

### 운영체제에서 메모리 관리란 무엇일까

운영체제에서 메모리 관리는 단순히 “메모리를 나눠주는 것”이 아니에요.

**CPU가 처리할 명령어와 데이터를 저장하는 메인 메모리를
어떻게 나누고, 보호하고, 효율적으로 사용할 것인가**를 책임지는
운영체제의 핵심 기능이에요.

프로그램은 처음부터 메모리에 존재하지 않아요.

HDD나 SSD 같은 **보조기억장치**에 저장돼 있다가,

프로그램이 실행되면 운영체제가

- 프로그램 코드
- 전역 데이터
- 실행에 필요한 정보

를 **메인 메모리(RAM)** 로 올려요.

이렇게 **메모리에 적재되어 실행 중인 프로그램 상태**를

⇒ **프로세스(Process)** 라고 해요.

### 메모리와 프로세스의 기본 개념

**메모리는 어떻게 생겼을까?**

메모리는 내부적으로 보면

⇒ **주소(Address)가 붙은 거대한 바이트(Byte) 배열**이에요.

- 각 바이트는 고유한 주소를 가져요
- CPU는 반드시 이 주소를 통해서만 메모리에 접근해요

즉, 메모리는 “데이터 덩어리”가 아니라 **주소 기반 접근 구조**예요.

**CPU와 메모리의 관계**

CPU는 아무 생각 없이 명령어를 실행하지 않아요.

CPU 안에는 **Program Counter(PC)** 라는 레지스터가 있고,

이 PC가 항상

> “다음에 실행할 명령어가 어디에 있는지”

를 가리키고 있어요.

실행 흐름은 다음과 같아요.

1. PC가 가리키는 주소에서 명령어를 가져와요 (Fetch)
2. 명령어를 해석하고 실행해요 (Decode & Execute)
3. 실행 과정에서 메모리에 데이터를 읽거나(load), 저장(store)해요
4. PC를 다음 명령어 주소로 갱신해요

⇒ 그래서 **메모리가 없으면 CPU는 아무 작업도 할 수 없어요.**

### 주소(Address)의 종류

운영체제 메모리 관리에서 **주소 개념은 핵심 중의 핵심**이에요.

**논리 주소 (Logical Address)**

- 프로세스가 사용하는 주소예요
- 각 프로세스는 **자기만의 독립적인 주소 공간**을 가져요
- 실제 메모리 위치와는 무관해요

⇒ 가상 주소(Virtual Address)라고도 불러요.

**상대 주소 (Relative Address)**

- 프로그램 시작 지점을 기준으로 계산한 주소예요
- “시작 주소 + 오프셋” 형태예요

⇒ 이 개념 덕분에 **재배치가 가능**해져요.

**물리 주소 (Physical Address)**

- 실제 RAM 하드웨어의 주소예요
- CPU가 최종적으로 접근하는 주소예요

**주소 변환은 누가 할까?**

논리 주소 → 물리 주소 변환은

운영체제가 아니라 **하드웨어(MMU, Memory Management Unit)** 가 담당해요.

⇒ 운영체제는 변환 규칙(페이지 테이블 등)을 설정하고,

⇒ 실제 변환은 MMU가 실시간으로 수행해요.

### 메모리 관리가 반드시 필요한 이유 (5대 요구사항)

현대 운영체제는 대부분 **멀티프로그래밍 환경**이에요.

즉, 여러 개의 프로세스가 **동시에 메모리에 올라가 실행**돼요.

이 환경에서 운영체제는 다음 요구사항을 반드시 만족해야 해요.

1. 재배치 (Relocation)

프로세스는 실행 중에

- 메모리에서 내려갔다가 (Swap-out)
- 다시 올라올 수 있어요 (Swap-in)

이때 항상 같은 주소에 올라와야 한다면 메모리 활용이 극도로 제한돼요.

그래서 프로세스는

**⇒ 메모리 내 어디로든 이동 가능해야 하고**,

⇒ 주소가 바뀌어도 정상 실행돼야 해요.

이걸 **재배치(Relocation)** 라고 해요.

1. 보호 (Protection)

어떤 프로세스도

- 다른 프로세스의 메모리
- 운영체제 커널 영역

을 침범하면 안 돼요.

이를 위해 하드웨어적으로

- **Base Register** (시작 주소)
- **Limit Register** (허용 크기)

를 사용해요.

CPU는 모든 메모리 접근 시 주소가 이 범위 안에 있는지 검사해요.

1. 공유 (Sharing)

여러 프로세스가

- 같은 프로그램
- 같은 라이브러리 코드

를 사용할 수 있어요.

이때 각자 복사본을 갖는 것보다

**⇒ 읽기 전용 코드 영역을 하나만 두고 공유**하는 게 훨씬 효율적이에요.

1. 논리적 구성 (Logical Organization)

프로그래머 입장에서 메모리는

- 코드
- 함수
- 배열
- 스택

같은 **논리적 단위**로 구성돼요.

이 요구를 가장 잘 만족하는 기법이

⇒ **세그멘테이션(Segmentation)** 이에요.

1. 물리적 구성 (Physical Organization)

실제 메모리는 한정돼 있고, 보조기억장치와의 데이터 이동도 필요해요.

이 복잡한 관리는 **프로그래머가 아니라 운영체제가 담당**해요.

### 연속 메모리 할당 방식

과거에는 프로세스를 메모리의 **연속된 공간에 통째로** 올렸어요.

**고정 분할 (Fixed Partitioning)**

- 메모리를 미리 정해진 크기의 파티션으로 분할
- 각 파티션에 하나의 프로세스 할당

문제점은 명확해요.

- 파티션보다 큰 프로세스는 실행 불가
- 작은 프로세스는 공간 낭비 발생

⇒ 이 낭비를 **내부 단편화**라고 해요.

**동적 분할 (Dynamic Partitioning)**

- 프로세스 크기만큼만 메모리를 할당해요

하지만 시간이 지나면

- 프로세스 생성/종료 반복
- 메모리 중간중간에 작은 빈 공간(Hole) 발생

⇒ 전체 공간은 충분한데 연속 공간이 없어 실행 못 하는 상황

⇒ 이게 **외부 단편화**예요.

**외부 단편화 해결: Compaction**

사용 중인 프로세스를 한쪽으로 몰아서 빈 공간을 하나로 합치는 작업이에요.

- 효과는 좋지만
- CPU 소모가 매우 커요

⇒ 실무에서는 거의 사용하지 않아요.

**배치 알고리즘**

- **First-fit**: 처음 발견한 공간
- **Next-fit**: 마지막 위치 이후부터
- **Best-fit**: 가장 비슷한 크기 공간

**버디 시스템 (Buddy System)**

- 메모리를 2^k 단위로 관리
- 필요 시 반으로 쪼개고
- 해제 시 다시 합침

⇒ 고정 + 동적 분할의 절충안이에요.

### 불연속 메모리 할당 방식

연속 할당의 단편화 문제를 해결하기 위해

⇒ **프로세스를 여러 조각으로 나눠 배치**하는 방식이 등장했어요.

**페이징 (Paging)**

- 프로세스 → 페이지(Page)
- 메모리 → 프레임(Frame)
- 크기 동일

이 구조 덕분에 **외부 단편화는 완전히 제거**돼요.

**주소 변환 과정**

논리 주소는 `(페이지 번호 p, 오프셋 d)` 형태예요.

MMU는 페이지 테이블을 통해

- p → 프레임 번호 f 변환
- 최종 물리 주소 `(f, d)` 생성

단점

- 페이지 마지막에서 내부 단편화 발생 가능
- 페이지 테이블이 커짐

해결

- 다단계 페이지 테이블
- 역 페이지 테이블(IPT)

**세그멘테이션 (Segmentation)**

- 논리적 단위로 분할
- 크기 가변

세그먼트 테이블에는 시작 주소(Base)와 길이(Length)가 저장돼요.

오프셋이 길이를 넘으면

⇒ 즉시 에러 → **강력한 보호 기능**

### 가상 메모리 (Virtual Memory)

가상 메모리는 **물리 메모리 크기와 상관없이
프로세스에게 큰 주소 공간을 제공하는 기술**이에요.

핵심 아이디어

1. 논리 주소와 물리 주소 분리
2. 프로그램 전체를 올릴 필요 없음
3. 필요한 부분만 메모리에 적재

**⇒ 부분 적재(Partial Loading)**

**요구 페이징 (Demand Paging)**

페이지에 접근할 때만 메모리에 적재해요.

페이지가 없으면

**⇒ 페이지 폴트(Page Fault)** 발생

운영체제가 개입해서 디스크에서 가져와요.

**프리페이징 (Prepaging)**

참조 지역성을 기반으로 인접 페이지까지 미리 적재해요.

### **페이지 교체 알고리즘 (Page Replacement Algorithm)**

가상 메모리를 사용하는 시스템에서는 프로그램이 필요로 하는 모든 페이지를 한 번에 메모리에 올릴 수 없어요.

그래서 **메모리가 가득 찬 상태에서 새로운 페이지를 가져와야 할 때**, 기존에 있던 페이지 중 하나를 내보내야 해요.

이때 어떤 페이지를 내보낼 것인가를 결정하는 기준이 **페이지 교체 알고리즘**이에요.

**페이지 교체 알고리즘**

- OPT
- FIFO
- LRU
- Clock

### OPT (Optimal Page Replacement)

OPT는 앞으로 가장 오랫동안 사용되지 않을 페이지를 교체하는 방식이에요.

이 방식은

- 페이지 폴트 횟수가 이론적으로 최소
- 가장 이상적인 알고리즘

이라는 장점이 있어요.

하지만 문제는, 미래에 어떤 페이지가 사용될지 알 수 없기 때문에 실제 시스템에서는 구현이 불가능해요.

그래서 OPT는 비교 기준(이론적 최적값)으로 사용돼요.

### FIFO (First-In First-Out)

FIFO는 가장 먼저 메모리에 들어온 페이지를 가장 먼저 내보내는 방식이에요.

구현이 매우 단순하고 직관적이에요.

하지만 FIFO는

- 페이지가 얼마나 자주 사용됐는지
- 지금도 사용 중인지

를 전혀 고려하지 않아요.

그래서 **자주 쓰이는 페이지도 오래됐다는 이유만으로 교체되는 문제**가 있고, **Belady의 역설**처럼 프레임 수를 늘려도 페이지 폴트가 증가하는 현상이 발생할 수 있어요.

### LRU (Least Recently Used)

LRU는 **가장 오랫동안 사용되지 않은 페이지를 교체**하는 방식이에요.

이 알고리즘의 핵심은

⇒ **지역성(Locality)** 이에요.

프로그램은 보통

- 최근에 사용한 데이터는 다시 사용할 가능성이 높고
- 오래 안 쓴 데이터는 앞으로도 안 쓸 가능성이 높아요.

LRU는 이 특성을 잘 반영해요.

그래서

- 실제 시스템의 접근 패턴과 잘 맞고
- 페이지 폴트 횟수도 적어요

하지만 단점은, **모든 페이지의 사용 시점을 추적해야 해서
구현 비용이 크다**는 점이에요.

그래서 완전한 LRU는 잘 쓰이지 않아요.

### Clock 알고리즘

Clock 알고리즘은 **LRU를 현실적으로 구현하기 위한 근사 알고리즘**이에요.

페이지마다

- 참조 비트(Use bit)

를 두고, 시계 바늘처럼 원형으로 페이지를 검사해요.

동작 방식은 다음과 같아요.

1. 참조 비트가 1이면

   → 최근에 사용된 페이지이므로

   → 비트를 0으로 바꾸고 넘어감

2. 참조 비트가 0이면

   → 최근에 사용되지 않았다고 판단

   → 해당 페이지를 교체

이 방식은

- LRU에 가까운 성능
- 훨씬 적은 오버헤드

를 가져서 현대 운영체제에서 매우 널리 사용돼요.

### Thrashing (쓰레싱)

Thrashing은 가상 메모리 시스템에서 가장 치명적인 성능 문제예요.

프로세스가 실행되면서

- 필요한 페이지가 계속 메모리에 없고
- 페이지 폴트가 과도하게 발생하면

CPU는

- 실제 연산은 거의 못 하고
- 페이지를 디스크로 내리고 가져오는 작업(Swapping)만 반복해요.

이 상태를 Thrashing 이라고 해요.

**Thrashing은 왜 발생할까?**

주된 원인은

- 프로세스에 할당된 프레임 수가 너무 적어서
- 작업에 필요한 페이지 집합을 메모리에 유지하지 못할 때예요.

이 경우

- 페이지 폴트 증가
- 디스크 I/O 폭증
- 전체 시스템 성능 급락

이라는 악순환이 발생해요.

해결방안

1. **Working Set Model**

Working Set은 **최근 일정 시간 동안 실제로 참조된 페이지들의 집합**이에요.

운영체제는

- 이 Working Set이 전부 메모리에 유지되도록
- 프레임 수를 조절해요

그래서 프로세스가 자기 작업에 필요한 최소한의 페이지를 항상 메모리에 가지도록 보장해요.

1. **Page Fault Frequency (PFF) 제어**

PFF는 **페이지 폴트 발생 빈도**를 기준으로 제어해요.

- 페이지 폴트가 너무 많으면
  ⇒ 프레임을 추가 할당
- 페이지 폴트가 너무 적으면
  ⇒ 프레임 회수

이 방식은 시스템 부하에 따라 동적으로 대응할 수 있어요.

### 프로세스 메모리 구조: 힙과 스택

프로세스가 메모리에 올라가면 일반적으로 다음과 같은 구조를 가져요.

- Code 영역
- Data 영역
- Heap 영역
- Stack 영역

각 영역은 **역할과 관리 방식이 완전히 달라요.**

Code 영역

- 실행할 기계어 명령어 저장
- 보통 읽기 전용
- 여러 프로세스가 공유 가능

Data 영역

- 전역 변수
- 정적 변수

프로그램 시작 시 할당되고 종료 시 해제돼요.

**스택(Stack)**

스택은 **함수 호출과 관련된 정보를 저장하는 공간**이에요.

- 함수 호출 시 스택 프레임 생성
- 지역 변수 저장
- 함수 종료 시 자동 해제

특징은

- LIFO 구조
- 접근 속도가 매우 빠름
- 크기 제한이 있음

⇒ 그래서 스택 오버플로우가 발생할 수 있어요.

**힙(Heap)**

힙은 **실행 중에 동적으로 메모리를 할당하는 공간**이에요.

- `malloc`, `new` 등으로 할당
- 개발자가 직접 해제해야 함

특징은

- 크기 유연
- 스택보다 느림
- 관리 책임이 개발자에게 있음

그래서

⇒ 메모리 누수

⇒ 단편화

⇒ 성능 저하

문제가 발생할 수 있어요.

힙 영역을 과도하게 키우면

- 스택 영역과 충돌할 수 있고
- 실제 사용하지 않는 메모리가 낭비돼요
- 페이지 수 증가로 페이지 폴트 가능성 증가

결과적으로 **성능 저하와 안정성 문제**가 발생해요.

### 10. 캐시 메모리와 DMA

캐시 메모리는 **CPU와 메인 메모리 사이의 속도 차이를 줄이기 위한 고속 메모리**예요.

CPU는 매우 빠른데 메모리는 상대적으로 느리기 때문에 그 차이를 완화해야 해요.

**캐시에서 가장 중요한 문제: `적중률(Hit Rate)`**

CPU가 필요한 데이터가

- 캐시에 있으면 → Cache Hit
- 없으면 → Cache Miss

⇒ 적중률이 높을수록 성능이 좋아져요.

**지역성**

적중률을 높일 수 있는 이유는 프로그램의 접근 패턴 때문이에요.

- 시간 지역성: 최근에 사용한 데이터는 다시 사용될 가능성이 큼
- 공간 지역성: 사용한 주소 주변도 곧 사용될 가능성이 큼

**TLB**

TLB는 **주소 변환을 위한 전용 캐시**예요.

자주 사용하는

- 가상 페이지 번호
- 물리 프레임 번호

매핑을 저장해서 페이지 테이블 접근을 줄여요.

⇒ 주소 변환 속도가 크게 빨라져요.

**DMA (Direct Memory Access)**

DMA는 **I/O 장치가 CPU 개입 없이 메모리에 직접 접근하는 방식**이에요.

이 방식 덕분에

- CPU는 다른 작업 수행 가능
- 전체 시스템 성능 향상

**Pinning**

DMA가 사용하는 메모리는

- 전송 중에
- 스왑 아웃되면 안 돼요.

그래서 운영체제는 해당 메모리를 **Pinning(고정)** 해서 디스크로 내려가지 않게 해요.
